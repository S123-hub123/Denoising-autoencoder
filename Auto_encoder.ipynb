{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "157d1632-e2f4-4ec8-81d4-ea5fb49a0feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task # 1: Build and train a denoising autoencoder (with at least 4 encoding/decoding\n",
      "layers) for a dataset of 100 grayscale natural images of size 64×64\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Task # 1: Build and train a denoising autoencoder (with at least 4 encoding/decoding\n",
    "layers) for a dataset of 100 grayscale natural images of size 64×64\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00a1710-8ac1-4589-92b5-2fbe96562e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101fb90a-cbb8-474b-8267-e7e305229028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 50  # Increased epochs\n",
    "learning_rate = 0.0005  # Adjusted learning rate\n",
    "image_size = 64  # Resize MNIST images from 28x28 to 64x64\n",
    "\n",
    "# 2. Create Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7167c7e-a2ef-4a47-bed5-ec2ce91efa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Autoencoder Model\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling the image\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f889e67-df94-46be-ba03-8a2175a1135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Autoencoder Model\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder: Downsampling the image\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder: Upsampling the image\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()  # Use sigmoid for the output layer to keep pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15938cc6-4dd6-445e-9847-f4f6b54095a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Adding Noise\n",
    "def add_uniform_noise(images):\n",
    "    \"\"\"Add uniform noise to the images.\"\"\"\n",
    "    noise = torch.rand_like(images) * 0.3  # Uniform noise in range [0, 0.3]\n",
    "    noisy_images = images + noise\n",
    "    return torch.clamp(noisy_images, 0., 1.)  # Clip pixel values to [0, 1]\n",
    "\n",
    "def add_gaussian_noise(images):\n",
    "    \"\"\"Add Gaussian noise to the images.\"\"\"\n",
    "    noise = torch.randn_like(images) * 0.2  # Gaussian noise with mean 0 and stddev 0.2\n",
    "    noisy_images = images + noise\n",
    "    return torch.clamp(noisy_images, 0., 1.)  # Clip pixel values to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3394e7e-4594-455a-b914-89ca65ddf122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualization and Saving Images\n",
    "def show_and_save_images(original, noisy, denoised, n=5, save_dir='./output'):\n",
    "    \"\"\"Plot original, noisy, and denoised images, and save them to files.\"\"\"\n",
    "    import os\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Set up plotting\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(9, n*3))\n",
    "\n",
    "    for i in range(n):\n",
    "        # Original images\n",
    "        axes[i, 0].imshow(original[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Original\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Noisy images\n",
    "        axes[i, 1].imshow(noisy[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[i, 1].set_title(\"Noisy\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Denoised images\n",
    "        axes[i, 2].imshow(denoised[i].squeeze().cpu().numpy(), cmap='gray')\n",
    "        axes[i, 2].set_title(\"Denoised\")\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(os.path.join(save_dir, 'comparison_images.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf88127-8382-4e40-92dd-a68ca9ecd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training and Testing Function\n",
    "def train_autoencoder(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            noisy_images = add_uniform_noise(images).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(noisy_images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "def test_autoencoder(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dataiter = iter(test_loader)\n",
    "        images, _ = next(dataiter)\n",
    "        noisy_images = add_gaussian_noise(images).to(device)\n",
    "        images = images.to(device)\n",
    "\n",
    "        outputs = model(noisy_images)\n",
    "\n",
    "        # Visualize original, noisy, and denoised images and save them to files\n",
    "        show_and_save_images(images, noisy_images, outputs, save_dir='./output_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e8de3-39ae-46bb-a437-2092d17cd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Main Execution\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DenoisingAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the autoencoder\n",
    "train_autoencoder(model, criterion, optimizer, train_loader, device)\n",
    "\n",
    "# Test the autoencoder and visualize results\n",
    "test_autoencoder(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca840ae-65c8-4e82-ba39-f8457bbdce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"Interpretation of Results:\n",
    "Loss Values: During training, the loss starts at 0.0549 and decreases steadily, indicating that the model is learning how to reconstruct images from noisy inputs.\n",
    "The low loss at the end of 50 epochs (0.0007) suggests that the model is successfully denoising the images.\n",
    "Visual Quality: After testing, the images should display a clear improvement from the noisy inputs to the denoised outputs, although some minor blurriness may remain depending on the model’s capacity.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcbde8-afb7-477b-9ec0-52a93d86499c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de7571-7d4a-41d9-a9ba-1d720cb49a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
